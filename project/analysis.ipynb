{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1eff7e",
   "metadata": {},
   "source": [
    "# Gendered Reception of Politicians in Online Political Discourse\n",
    "\n",
    "### Computational Social Sciences\n",
    "\n",
    "**Authors:** Tudor, Salome, Miguel, Micaela, Mathilde  \n",
    "**Course:** Computational Social Sciences (2025–2026)\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Online social media platforms have become central arenas for political debate, shaping public perception of political actors. Previous research suggests that women in politics are subject to different forms of evaluation and criticism than their male counterparts, often involving gendered language and personal attacks.\n",
    "\n",
    "In this project, we investigate how the online reception of female politicians differs from that of male politicians using computational methods. Relying on publicly available datasets of political discussions on social media, we analyze textual content and interaction structures to identify differences in sentiment, toxicity, thematic focus, and network dynamics.\n",
    "\n",
    "By combining natural language processing techniques with graph-based analysis, this study aims to contribute to a better understanding of gender bias in online political discourse while critically reflecting on the methodological and ethical limitations of computational approaches.\n",
    "\n",
    "\n",
    "1. Invisible Women in Digital Diplomacy: A Multidimensional Framework for Online Gender Bias Against Women Ambassadors Worldwide [https://arxiv.org/abs/2311.17627?utm]\n",
    "2. Gender Differences in Abuse: The Case of Dutch Politicians on Twitter [https://arxiv.org/abs/2306.10769?utm]\n",
    "3. Gender stereotypes in politics: Insights from social media data [https://www.sciencedirect.com/science/article/abs/pii/S016517652500240X?utm]\n",
    "4. Polarization and hate speech with gender bias associated with politics: analysis of interactions on Twitter [https://www.redalyc.org/journal/5894/589475507003/?utm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50adbc0",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Social media platforms play a central role in contemporary political communication, allowing direct interaction between political figures and the public. While these platforms can foster political engagement, they also expose public figures to large volumes of unmoderated commentary, including harassment and hate speech.\n",
    "\n",
    "Gender bias in political communication has been documented in traditional media, where women are often evaluated based on personal attributes rather than political positions. Online platforms introduce additional dynamics such as anonymity, virality, and network effects, which may amplify these biases.\n",
    "\n",
    "In this project, we focus on the online reception of selected French political figures, primarily Marine Le Pen and Emmanuel Macron, with additional data on a male politician from a similar ideological background. Using computational tools, we aim to systematically analyze large-scale online discourse and examine how gender and political alignment intersect in shaping public perception and online commentary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a299ea",
   "metadata": {},
   "source": [
    "## 2. Research Question and Hypotheses\n",
    "\n",
    "### Research Question\n",
    "\n",
    "How does the online reception of Marine Le Pen differ from that of Emmanuel Macron on Twitter, and to what extent can observed differences be associated with gender versus political alignment?\n",
    "\n",
    "### Sub-questions\n",
    "\n",
    "- Are tweets referring to Marine Le Pen more negative or toxic than those referring to Emmanuel Macron?  \n",
    "- Do the dominant topics differ between discussions about these politicians?  \n",
    "- Are gendered or personal themes (e.g. appearance, legitimacy, personal life) more prevalent in tweets about Marine Le Pen?  \n",
    "- How does the reception of Marine Le Pen compare to that of a male politician from a similar ideological background?\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **H1:** Tweets referring to Marine Le Pen exhibit higher levels of toxicity and personal attacks than those referring to Emmanuel Macron.  \n",
    "- **H2:** Topic modeling reveals gender-specific themes, with tweets about Marine Le Pen more frequently referencing appearance or personal attributes.  \n",
    "- **H3:** Differences in toxicity and interaction patterns persist, though are partially reduced, when comparing Marine Le Pen to a male politician from a similar political orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a49faa",
   "metadata": {},
   "source": [
    "## 3. Methodological Overview\n",
    "\n",
    "To address the research questions, we adopt a computational approach combining text analysis and network analysis.\n",
    "\n",
    "### Methods\n",
    "\n",
    "- **Natural Language Processing (NLP):**\n",
    "  - Text preprocessing\n",
    "  - Sentiment and toxicity classification using pre-trained models\n",
    "  - Topic modeling to uncover dominant themes in discussions\n",
    "\n",
    "- **Graph Analysis:**\n",
    "  - Construction of reply or interaction networks\n",
    "  - Analysis of centrality, clustering, and coordination patterns\n",
    "\n",
    "### Computational Social Science Perspective\n",
    "\n",
    "While computational methods enable large-scale analysis, they are not neutral. Automated classifiers may encode social biases, and network structures may reflect platform-specific affordances. These limitations are explicitly discussed in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c988e3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\micag\\\\anaconda3\\\\envs\\\\css_full\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "# INSTALL ALL OF THIS LIBRARIES BEFORE RUNNING THE CODE\n",
    "# %pip install pandas numpy matplotlib seaborn nltk scikit-learn networkx transformers torch wordcloud textblob kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2ed68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting detoxify\n",
      "  Using cached detoxify-0.5.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from detoxify) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from detoxify) (2.3.1)\n",
      "Collecting sentencepiece>=0.1.94 (from detoxify)\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (2025.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from torch>=1.7.0->detoxify) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.7.0->detoxify) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.7.0->detoxify) (2021.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from jinja2->torch>=1.7.0->detoxify) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from sympy->torch>=1.7.0->detoxify) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from transformers->detoxify) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from tqdm>=4.27->transformers->detoxify) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from requests->transformers->detoxify) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from requests->transformers->detoxify) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from requests->transformers->detoxify) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micag\\anaconda3\\envs\\css_full\\lib\\site-packages (from requests->transformers->detoxify) (2026.1.4)\n",
      "Using cached detoxify-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 8.4 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece, detoxify\n",
      "\n",
      "   ---------------------------------------- 2/2 [detoxify]\n",
      "\n",
      "Successfully installed detoxify-0.5.2 sentencepiece-0.2.1\n",
      "Environment ready\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas numpy matplotlib seaborn nltk scikit-learn networkx transformers torch wordcloud textblob kagglehub detoxify\n",
    "!pip install detoxify\n",
    "\n",
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Topic modeling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Graphs\n",
    "import networkx as nx\n",
    "\n",
    "# Utils\n",
    "from collections import Counter\n",
    "\n",
    "# Kagglehub for dataset download\n",
    "import kagglehub\n",
    "\n",
    "# install torch and after, transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import detoxify\n",
    "\n",
    "print(\"Environment ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562d7825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\micag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\micag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac73566",
   "metadata": {},
   "source": [
    "## 4. Data Sources\n",
    "\n",
    "Due to access restrictions and recent policy changes affecting the Twitter/X API, this project relies on **publicly available datasets** commonly used in computational social science research.\n",
    "\n",
    "The datasets consist of social media posts and replies related to political actors during specific political events. Each observation includes textual content and interaction metadata, enabling both linguistic and network-based analyses.\n",
    "\n",
    "Using open datasets ensures reproducibility, transparency, and ethical compliance, while still allowing us to study real-world political discourse at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "\n",
    "path_2017 = kagglehub.dataset_download(\"jeanmidev/french-presidential-election\")\n",
    "path_2022 = kagglehub.dataset_download(\"jeanmidev/french-presidential-online-listener\")\n",
    "\n",
    "path_2022 = os.path.join(path_2022, \"2022/twitter\")\n",
    "\n",
    "print(\"Path to dataset 2017 files:\", path_2017)\n",
    "print(\"Path to dataset 2022 files:\", path_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe264b",
   "metadata": {},
   "source": [
    "Import the 2017 databases into a dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2321ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_googletrends = os.path.join(path_2017, \"database_googletrends.sqlite\")\n",
    "if (os.path.exists(path_googletrends)):\n",
    "    os.remove(path_googletrends)\n",
    "\n",
    "dfs_dict_2017 = {}\n",
    "mention_2017=['\"mention_Macron\"', '\"mention_Le PEN\"']\n",
    "\n",
    "for m in mention_2017:\n",
    "    for file in os.listdir(path_2017):\n",
    "        # print(\"File in dataset:\", file)\n",
    "\n",
    "        file_path = os.path.join(path_2017, file)\n",
    "        # print(\"Loading file:\", file_path)\n",
    "\n",
    "        connection = sqlite3.connect(file_path)\n",
    "        # print(\"Reading the database\")\n",
    "\n",
    "        sql_query = f\"SELECT text, day, hour FROM data WHERE lang='en' AND \"+m+\"=1;\"\n",
    "        if m not in dfs_dict_2017:\n",
    "            dfs_dict_2017[m] = []\n",
    "        dfs_dict_2017[m].append(pd.read_sql_query(sql_query, connection))\n",
    "\n",
    "        connection.close()\n",
    "\n",
    "# concat tweets of all database on one list\n",
    "for m in mention_2017:\n",
    "    dfs_dict_2017[m] = pd.concat(dfs_dict_2017[m], ignore_index=True)\n",
    "\n",
    "print(dfs_dict_2017)\n",
    "print(\"Finalized loading 2017 databases into dataframes dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e3dc8",
   "metadata": {},
   "source": [
    "Import 2022 databases into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c78cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict_2022 = {}\n",
    "mention_2022 = ['macron', 'pen']\n",
    "\n",
    "for file in os.listdir(path_2022):\n",
    "    print(\"File in dataset:\", file)\n",
    "    file_path = os.path.join(path_2022, file)\n",
    "\n",
    "    # Read the CSV file\n",
    "    df_temp = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    for m in mention_2022:\n",
    "        # Filter rows containing \"macron\" or \"pen\" (case-insensitive)\n",
    "        # This checks all string columns for the keywords\n",
    "        mask = df_temp.astype(str).apply(\n",
    "            lambda x: x.str.contains(m, case=False, na=False)\n",
    "        ).any(axis=1)\n",
    "\n",
    "        if m not in dfs_dict_2022:\n",
    "            dfs_dict_2022[m] = []\n",
    "\n",
    "        dfs_dict_2022[m].append(df_temp[mask])\n",
    "\n",
    "\n",
    "print(dfs_dict_2022)\n",
    "for m in mention_2022:\n",
    "    dfs_dict_2022[m] = pd.concat(dfs_dict_2022[m], ignore_index=True)\n",
    "\n",
    "print(dfs_dict_2022)\n",
    "print(\"Finalized loading 2022 csv into dataframes dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, we create an example dataset\n",
    "data = {\n",
    "    \"comment_id\": [1,2,3,4,5],\n",
    "    \"text\": [\n",
    "        \"I really support Marine Le Pen, she's amazing!\",\n",
    "        \"Jordan Bardella's policy ideas are terrible\",\n",
    "        \"Marine Le Pen is strong but looks unfriendly\",\n",
    "        \"Bardella seems competent but inexperienced\",\n",
    "        \"Le Pen is awful, can't stand her\"\n",
    "    ],\n",
    "    \"target_politician\": [\"Marine Le Pen\",\"Jordan Bardella\",\"Marine Le Pen\",\"Jordan Bardella\",\"Marine Le Pen\"],\n",
    "    \"gender\": [\"F\",\"M\",\"F\",\"M\",\"F\"],\n",
    "    \"timestamp\": [\"2026-01-01 12:00\",\"2026-01-01 12:05\",\"2026-01-01 12:10\",\"2026-01-01 12:15\",\"2026-01-01 12:20\"],  # Keep as strings\n",
    "    \"reply_to\": [None, 1, None, 2, None],\n",
    "}\n",
    "\n",
    "for m in mention_2017:\n",
    "    # Get the corresponding dataframe from dfs_dict\n",
    "    df_mention = dfs_dict_2017[m]\n",
    "\n",
    "    for idx, row in df_mention.iterrows():\n",
    "        data[\"comment_id\"].append(len(data[\"comment_id\"])+1)\n",
    "        data[\"text\"].append(row[\"text\"])  # Get text from dfs_dict\n",
    "\n",
    "        if m == '\"mention_Le PEN\"':\n",
    "            data[\"target_politician\"].append(\"Marine Le Pen\")\n",
    "            data[\"gender\"].append(\"F\")\n",
    "        else:\n",
    "            data[\"target_politician\"].append(\"Emmanuel Macron\")\n",
    "            data[\"gender\"].append(\"M\")\n",
    "\n",
    "        data[\"reply_to\"].append(None)\n",
    "\n",
    "        # Create timestamp from day and hour\n",
    "        timestamp_str = f\"{row['day']} {row['hour']}:00:00\"\n",
    "        data[\"timestamp\"].append(timestamp_str)\n",
    "\n",
    "for m in mention_2022:\n",
    "    df_mention = dfs_dict_2022[m]\n",
    "\n",
    "    for idx, row in df_mention.iterrows():\n",
    "        # Skip rows where 'text' is not a string\n",
    "        if isinstance(row['text'], str):\n",
    "            data[\"comment_id\"].append(len(data[\"comment_id\"])+1)\n",
    "            data[\"text\"].append(row[\"text\"])  # Get text from dfs_dict\n",
    "\n",
    "            if m == 'pen':\n",
    "                data[\"target_politician\"].append(\"Marine Le Pen\")\n",
    "                data[\"gender\"].append(\"F\")\n",
    "            else:\n",
    "                data[\"target_politician\"].append(\"Emmanuel Macron\")\n",
    "                data[\"gender\"].append(\"M\")\n",
    "\n",
    "            data[\"reply_to\"].append(None)\n",
    "            # Ensure pure string before conversion\n",
    "            data[\"timestamp\"].append(str(row['created_date']))\n",
    "\n",
    "# # Convert timestamps with explicit format, otherwise it will crash\n",
    "converted_ts = pd.to_datetime(data[\"timestamp\"], errors='coerce', format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"timestamp\"] = converted_ts\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a78fd",
   "metadata": {},
   "source": [
    "Save the df to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"le_pen_macron_twitter_2017_2022_toxicity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801dde3",
   "metadata": {},
   "source": [
    "Import the csv to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"le_pen_macron_twitter_2017_2022_toxicity.csv\")\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors='coerce', format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33347114",
   "metadata": {},
   "source": [
    "Downsample the dataframe from 16 million to 0.8 million rows for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77682179",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLE_RATIO = 0.05  # 5% of 16 million is 0.8 million\n",
    "df = df.sample(frac=DOWNSAMPLE_RATIO, random_state=42)  # Downsample to 0.8 million rows\n",
    "\n",
    "print(\"Dataframe shape after downsampling:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce3269",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Preprocessing\n",
    "\n",
    "Before running any NLP or graph analyses, we perform basic preprocessing to unify all the dataset:\n",
    "\n",
    "- Convert text to lowercase\n",
    "- Remove punctuation and special characters\n",
    "- Tokenize text\n",
    "- Remove stopwords\n",
    "- Optional: lemmatization/stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "## observation: to start it is not needed but after\n",
    "## i want to add a part that considers that some comments will refer\n",
    "## only to the last name of the politician, and not the complete name\n",
    "# code included in the annexe for the moment\n",
    "# 1. Import the library\n",
    "import spacy\n",
    "\n",
    "# 2. Load a language model (commonly assigned to the variable 'nlp')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s']\", \"\", text)  # allows letters, spaces and '\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # adjust if it's French\n",
    "\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()  # simple tokenize\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "df[[\"text\",\"clean_text\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48932e92",
   "metadata": {},
   "source": [
    "Version for Mac OS -> nlp doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get stopwords\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Example usage\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11435f22",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before performing NLP or network analyses, we check:\n",
    "\n",
    "- Number of comments per politician\n",
    "- Number of comments per gender\n",
    "- Distribution of comment length\n",
    "- Simple word frequency counts to see dominant terms\n",
    "\n",
    "This helps us understand the dataset and spot potential biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments per politician\n",
    "print(\"Comments per politician:\")\n",
    "print(df.groupby(\"target_politician\").size())\n",
    "\n",
    "# Comments per gender\n",
    "print(\"\\nComments per gender:\")\n",
    "print(df.groupby(\"gender\").size())\n",
    "\n",
    "# Text length\n",
    "df[\"text_length\"] = df[\"clean_text\"].apply(lambda x: len(x.split()))\n",
    "print(\"\\nComment lengths:\")\n",
    "print(df[[\"target_politician\",\"text_length\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe0cee",
   "metadata": {},
   "source": [
    "- Graph the number of comments per election period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the total negative comments Macron vs Le Penn over a time period.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the period you want (start/end). Adjust as needed.\n",
    "start, end = \"2021-10-10\", \"2023-12-10\"\n",
    "mask = df[\"timestamp\"].between(start, end)\n",
    "\n",
    "# Counts per week per politician\n",
    "weekly_counts = (\n",
    "    df.loc[mask]\n",
    "      .set_index(\"timestamp\")\n",
    "      .groupby(\"target_politician\")\n",
    "      .resample(\"W\")\n",
    "      .size()\n",
    "      .unstack(0)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "print(weekly_counts)\n",
    "\n",
    "# Quick plot\n",
    "weekly_counts.plot(figsize=(8, 4), marker=\"o\")\n",
    "plt.title(\"Comments per week\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Set the period you want (start/end). Adjust as needed.\n",
    "start, end = \"2017-01-10\", \"2017-10-10\"\n",
    "mask = df[\"timestamp\"].between(start, end)\n",
    "\n",
    "# Counts per week per politician\n",
    "weekly_counts = (\n",
    "    df.loc[mask]\n",
    "      .set_index(\"timestamp\")\n",
    "      .groupby(\"target_politician\")\n",
    "      .resample(\"W\")\n",
    "      .size()\n",
    "      .unstack(0)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "print(weekly_counts)\n",
    "\n",
    "# Quick plot\n",
    "weekly_counts.plot(figsize=(8, 4), marker=\"o\")\n",
    "plt.title(\"Comments per week\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da31a8",
   "metadata": {},
   "source": [
    "- Top words per politician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for politician in df[\"target_politician\"].unique():\n",
    "    # Filter comments for this politician\n",
    "    politician_comments = df[df[\"target_politician\"] == politician]\n",
    "\n",
    "    # Concatenate all text and count words\n",
    "    all_words = \" \".join(politician_comments[\"clean_text\"].fillna(\"\")).split()\n",
    "    word_freq = Counter(all_words)\n",
    "\n",
    "    # Get top 10\n",
    "    top_words = word_freq.most_common(10)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Top 10 words for {politician}:\", top_words)\n",
    "\n",
    "    # Plotting\n",
    "    words, counts = zip(*top_words)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=list(counts), y=list(words), palette=\"viridis\")\n",
    "    plt.title(f\"Top 10 words for {politician}\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e0d5b",
   "metadata": {},
   "source": [
    "# 7. NLP Analysis\n",
    "- Sentiment analysis using HuggingFace model\n",
    "- Toxicity classification using detoxify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# see pythorch version\n",
    "print(\"PyThorch version:\", torch.__version__)\n",
    "\n",
    "# take into consideration macbooks\n",
    "device = 0\n",
    "if not (torch.cuda.is_available()):\n",
    "    device = -1\n",
    "    import os\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\",\n",
    "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                      device=device)\n",
    "\n",
    "print(classifier(\"Me encanta aprender data science!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ceeef0",
   "metadata": {},
   "source": [
    "- Sentiment analysis reference:\n",
    "\n",
    "    https://huggingface.co/blog/sentiment-analysis-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis using HuggingFace transformers\n",
    "import time\n",
    "\n",
    "# # We create a sentiment analysis pipeline\n",
    "# # If you want more specific toxicity detection, there are models like \"unitary/toxic-bert\" on HuggingFace\n",
    "# sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# # Apply to the dataframe (only a few examples)\n",
    "# df[\"sentiment\"] = df[\"clean_text\"].apply(lambda x: sentiment_model(x)[0][\"label\"])\n",
    "# df[\"sentiment_score\"] = df[\"clean_text\"].apply(lambda x: sentiment_model(x)[0][\"score\"])\n",
    "\n",
    "# df[[\"text\",\"clean_text\",\"sentiment\",\"sentiment_score\"]]\n",
    "\n",
    "# Optimize to use cuda batch processing if available; uses 70% of GPU and 90% of it's memory\n",
    "use_gpu = torch.cuda.is_available()\n",
    "sentiment_model = None\n",
    "try:\n",
    "    sentiment_model = classifier  # defined earlier in the earlier cell\n",
    "except NameError:\n",
    "    sentiment_model = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        device=0 if use_gpu else -1,\n",
    "        torch_dtype=torch.float16 if use_gpu else None,\n",
    "    )\n",
    "\n",
    "# Prepare inputs as a plain list of strings\n",
    "texts = df[\"clean_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# Batched, chunked inference with truncation & padding for speed\n",
    "batch_size = 128  # adjust based on RAM/GPU memory (64–256 is typical)\n",
    "\n",
    "results = []\n",
    "time_start = time.time()\n",
    "batch = texts[0:batch_size]\n",
    "out = sentiment_model(\n",
    "    batch,\n",
    "    batch_size=batch_size,     # pipeline supports batch_size on list input\n",
    "    truncation=True,           # cap long inputs\n",
    "    padding=True               # pad to speed up batch collation\n",
    ")\n",
    "results.extend(out)\n",
    "time_elapsed = time.time() - time_start\n",
    "\n",
    "print(f\"Processed first batch of {batch_size} in {time_elapsed:.2f} seconds.\")\n",
    "print(f\"Estimated total time for {len(texts)} comments: {((time_elapsed / batch_size) * len(texts))/60:0.2f} minutes.\")\n",
    "\n",
    "for i in range(batch_size, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    out = sentiment_model(\n",
    "        batch,\n",
    "        batch_size=batch_size,     # pipeline supports batch_size on list input\n",
    "        truncation=True,           # cap long inputs\n",
    "        padding=True               # pad to speed up batch collation\n",
    "    )\n",
    "    results.extend(out)\n",
    "\n",
    "# Assign outputs\n",
    "df[\"sentiment\"] = [r[\"label\"] for r in results]\n",
    "df[\"sentiment_score\"] = [r[\"score\"] for r in results]\n",
    "\n",
    "# Quick preview\n",
    "df[[\"text\", \"clean_text\", \"sentiment\", \"sentiment_score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec67b6e",
   "metadata": {},
   "source": [
    "- Toxicity analysis references:\n",
    "    https://huggingface.co/unitary/toxic-bert\n",
    "\n",
    "    https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4790c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxicity analysis example with detoxify\n",
    "from detoxify import Detoxify\n",
    "\n",
    "# select between cuda and cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Initialize the model; original - toxicity challenge model\n",
    "# returns a label - 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    "toxicity_model = Detoxify('original', device=device)\n",
    "\n",
    "# Prepare inputs as a plain list of strings\n",
    "texts = df[\"clean_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# adjust based on RAM/GPU memory (64–256 is typical)\n",
    "batch = 128\n",
    "\n",
    "results = []\n",
    "time_start = time.time()\n",
    "out = toxicity_model.predict(texts[0:batch_size])\n",
    "\n",
    "results.extend([\n",
    "    {\n",
    "        \"toxicity\": out[\"toxicity\"][i],\n",
    "        \"severe_toxicity\": out[\"severe_toxicity\"][i],\n",
    "        \"obscene\": out[\"obscene\"][i],\n",
    "        \"threat\": out[\"threat\"][i],\n",
    "        \"insult\": out[\"insult\"][i],\n",
    "        \"identity_attack\": out[\"identity_attack\"][i]\n",
    "    }\n",
    "    for i in range(len(out[\"toxicity\"]))\n",
    "])\n",
    "\n",
    "time_elapsed = time.time() - time_start\n",
    "print(f\"Processed first batch of {batch_size} in {time_elapsed:.2f} seconds.\")\n",
    "print(f\"Estimated total time for {len(texts)} comments: {((time_elapsed / batch_size) * len(texts))/60:0.2f} minutes.\")\n",
    "\n",
    "for i in range(batch_size, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    out = toxicity_model.predict(batch)\n",
    "    results.extend([\n",
    "        {\n",
    "            \"toxicity\": out[\"toxicity\"][j],\n",
    "            \"severe_toxicity\": out[\"severe_toxicity\"][j],\n",
    "            \"obscene\": out[\"obscene\"][j],\n",
    "            \"threat\": out[\"threat\"][j],\n",
    "            \"insult\": out[\"insult\"][j],\n",
    "            \"identity_attack\": out[\"identity_attack\"][j]\n",
    "        }\n",
    "        for j in range(len(out[\"toxicity\"]))\n",
    "    ])\n",
    "\n",
    "# Assign outputs\n",
    "df[\"toxicity\"]        = [r[\"toxicity\"] for r in results]\n",
    "df[\"severe_toxicity\"] = [r[\"severe_toxicity\"] for r in results]\n",
    "df[\"obscene\"]         = [r[\"obscene\"] for r in results]\n",
    "df[\"threat\"]          = [r[\"threat\"] for r in results]\n",
    "df[\"insult\"]          = [r[\"insult\"] for r in results]\n",
    "df[\"identity_attack\"] = [r[\"identity_attack\"] for r in results]\n",
    "\n",
    "# Quick preview\n",
    "df[[\"text\", \"clean_text\", \"sentiment\", \"sentiment_score\", \"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_wordcloud(texts):\n",
    "    text = \" \".join(texts)\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\"\n",
    "    ).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "plot_wordcloud(df[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2585a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#représentation TF-IDF afin de pondérer les termes en fonction de leur importance relative dans chaque tweet et de leur rareté dans le data set -> permet une comparaison quantitative entre tweets et groupes de tweets\n",
    "\n",
    "def compute_tfidf(texts, max_features=1000):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer\n",
    "\n",
    "positive = df[df[\"sentiment\"] == \"POSITIVE\"][\"clean_text\"] # ou avant/après une date -> utile pour les evenements par exemple\n",
    "negative = df[df[\"sentiment\"] == \"NEGATIVE\"][\"clean_text\"]\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "pos_idx = positive.index\n",
    "neg_idx = negative.index\n",
    "\n",
    "tfidf_pos = X[pos_idx].mean(axis=0).A1\n",
    "tfidf_neg = X[neg_idx].mean(axis=0).A1\n",
    "\n",
    "delta = tfidf_pos - tfidf_neg\n",
    "\n",
    "top_pos = feature_names[np.argsort(delta)[-10:]]\n",
    "top_neg = feature_names[np.argsort(delta)[:10]]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = np.concatenate([top_neg, top_pos])\n",
    "scores = np.concatenate([\n",
    "    delta[np.argsort(delta)[:10]],\n",
    "    delta[np.argsort(delta)[-10:]]\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.barh(words, scores)\n",
    "plt.axvline(0)\n",
    "plt.title(\"Mots discriminants (TF-IDF)\")\n",
    "plt.xlabel(\"TF-IDF positif  ←  →  négatif\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tweets_mar_may_2017_continuous(df, date_col):\n",
    "    # Conversion en datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "    # Filtrer uniquement l'année 2017\n",
    "    df_2017 = df[df[date_col].dt.year == 2017].copy()\n",
    "\n",
    "    # Filtrer entre mars et mai\n",
    "    df_period = df_2017[(df_2017[date_col].dt.month >= 3) &\n",
    "                        (df_2017[date_col].dt.month <= 5)]\n",
    "\n",
    "    # Créer une colonne 'day' pour le regroupement\n",
    "    df_period['day'] = df_period[date_col].dt.date\n",
    "\n",
    "    # Compter le nombre de tweets par jour\n",
    "    counts = df_period.groupby('day').size()\n",
    "\n",
    "    # Créer un index de tous les jours entre mars et mai\n",
    "    all_days = pd.date_range(start='2017-03-01', end='2017-05-31', freq='D')\n",
    "\n",
    "    # Réindexer pour inclure les jours sans tweet\n",
    "    counts = counts.reindex(all_days.date, fill_value=0)\n",
    "\n",
    "    # Tracer la courbe\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(counts.index, counts.values, marker='o')\n",
    "    plt.xlabel(\"Jour\")\n",
    "    plt.ylabel(\"Nombre de tweets\")\n",
    "    plt.title(\"Nombre de tweets par jour (mars → mai 2017)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors='coerce')\n",
    "df = df.sort_values(\"timestamp\")  # Assure un ordre chronologique\n",
    "\n",
    "plot_tweets_mar_may_2017(df, \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "df[\"sentiment\"] = df[\"clean_text\"].apply(sentiment_score)\n",
    "df[\"sentiment\"].hist(bins=30)\n",
    "plt.title(\"Distribution des sentiments\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9701cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df[\"vader_sentiment\"] = df[\"clean_text\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "df[\"vader_sentiment\"].hist(bins=30)\n",
    "plt.title(\"Distribution des scores VADER\")\n",
    "plt.show()\n",
    "\n",
    "# Vader is specifically designed for social media text and provides a compound score ranging from -1 (most negative) to +1 (most positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e49044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = liste de tweets, y = labels (0=non toxique, 1=toxique)\n",
    "X = df[\"clean_text\"]\n",
    "y = (df[\"sentiment\"] == \"NEGATIVE\").astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "score = model.score(X_test_vec, y_test)\n",
    "print(f\"Précision : {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Initialiser VADER pour l'analyse de toxicité\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Listes de pronoms et mots genrés\n",
    "pronoms_feminin = [\"elle\", \"elles\", \"la\", \"l'\", \"une\", \"celle\", \"celles\", \"madame\", \"mme\", \"femme\", \"dame\", \"mère\", \"sœur\"]\n",
    "pronoms_masculin = [\"il\", \"ils\", \"le\", \"l'\", \"un\", \"celui\", \"ceux\", \"monsieur\", \"mr\", \"homme\", \"garçon\", \"père\", \"frère\"]\n",
    "pronoms_pluriel = [\"ils\", \"elles\", \"eux\", \"les\", \"leurs\", \"mesdames\", \"messieurs\"]\n",
    "\n",
    "# Fonction pour détecter le genre\n",
    "def detecter_genre(texte):\n",
    "    texte = texte.lower()\n",
    "    count_feminin = sum(len(re.findall(rf\"\\b{re.escape(pronom)}\\b\", texte)) for pronom in pronoms_feminin)\n",
    "    count_masculin = sum(len(re.findall(rf\"\\b{re.escape(pronom)}\\b\", texte)) for pronom in pronoms_masculin)\n",
    "    count_pluriel = sum(len(re.findall(rf\"\\b{re.escape(pronom)}\\b\", texte)) for pronom in pronoms_pluriel)\n",
    "\n",
    "    if count_feminin > count_masculin and count_feminin > count_pluriel:\n",
    "        return \"féminin\"\n",
    "    elif count_masculin > count_feminin and count_masculin > count_pluriel:\n",
    "        return \"masculin\"\n",
    "    elif count_pluriel > 0:\n",
    "        return \"pluriel\"\n",
    "    else:\n",
    "        return \"neutre\"\n",
    "\n",
    "# Fonction pour analyser la toxicité avec VADER\n",
    "def analyser_toxicite(texte):\n",
    "    sentiment = analyzer.polarity_scores(texte)\n",
    "    return sentiment[\"compound\"]  # Score entre -1 (toxique) et +1 (positif)\n",
    "\n",
    "# Appliquer les fonctions à chaque tweet\n",
    "df[\"genre\"] = df[\"clean_text\"].apply(detecter_genre)\n",
    "df[\"toxicite\"] = df[\"clean_text\"].apply(analyser_toxicite)\n",
    "\n",
    "# Classer la toxicité (ex : toxique si score < -0.3)\n",
    "df[\"est_toxique\"] = df[\"toxicite\"] < -0.3\n",
    "\n",
    "# Sauvegarder les résultats\n",
    "df.to_csv(\"tweets_analyses.csv\", index=False)\n",
    "\n",
    "# Afficher un aperçu\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056abf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is not working correctly right now, needs more adjustments\n",
    "## maybe it would be useful to replace politician names with ids as said before\n",
    "# look in the annexe\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Vectorizar textos\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=1, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# mostrar temas\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx+1}: \", [feature_names[i] for i in topic.argsort()[-no_top_words:]])\n",
    "\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976987bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first test with a real database\n",
    "## tried to install kaggle package and download dataset from kaggle\n",
    "## didnt work but internet says that it is possible because its an old competition\n",
    "## ended up downloading it manually\n",
    "\n",
    "'''\n",
    "# instalar paquete kaggle\n",
    "%pip install kaggle\n",
    "\n",
    "# si no pusiste kaggle.json en la carpeta .kaggle, podés crearlo desde el notebook\n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"\"\n",
    "os.environ['KAGGLE_KEY'] = \"\"\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# crear carpeta data si no existe\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# iniciar API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# descargar dataset completo (zip)\n",
    "api.competition_download_files(\n",
    "    \"jigsaw-toxic-comment-classification-challenge\",\n",
    "    path=\"data\"\n",
    ")\n",
    "\n",
    "# el archivo se llama algo así: jigsaw-toxic-comment-classification-challenge.zip\n",
    "\n",
    "# descargamos el dataset completo (zip)\n",
    "kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -p ./data\n",
    "\n",
    "# descomprimir\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"./data/jigsaw-toxic-comment-classification-challenge.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")\n",
    "\n",
    "df_real = pd.read_csv(\"./data/train.csv\")\n",
    "df_real.head()\n",
    "\n",
    "'''\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "# lista todos los archivos zip en data\n",
    "zip_files = [f for f in os.listdir(data_folder) if f.endswith(\".zip\")]\n",
    "\n",
    "# descomprimir cada uno\n",
    "for z in zip_files:\n",
    "    with zipfile.ZipFile(os.path.join(data_folder, z), \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d054489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Exploration\n",
    "\n",
    "# load dataset\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# columnas de etiquetas\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# mostrar primeras filas\n",
    "print(train.head())\n",
    "\n",
    "# contar y mostrar distribución de cada etiqueta\n",
    "label_counts = {}\n",
    "for col in labels:\n",
    "    counts = train[col].value_counts()\n",
    "    label_counts[col] = counts\n",
    "    print(f\"{col}:\\n{counts}\\n\")\n",
    "\n",
    "# convertir a dataframe para graficar\n",
    "df_counts = pd.DataFrame({col: train[col].value_counts() for col in labels})\n",
    "df_counts = df_counts.fillna(0).astype(int)  # asegurarse de que no haya NaN\n",
    "df_counts = df_counts.T  # para que las etiquetas estén en filas\n",
    "df_counts.columns = ['No', 'Yes']  # 0 = No, 1 = Sí\n",
    "\n",
    "# gráfico de barras apiladas\n",
    "df_counts.plot(kind='bar', stacked=True, figsize=(10,6), color=['skyblue', 'salmon'])\n",
    "plt.title(\"Distribution of labels in the training dataset\")\n",
    "plt.ylabel(\"Number of comments\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Toxic', loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ddd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start we do binary classification: toxic vs non-toxic\n",
    "\n",
    "X = train[\"comment_text\"]\n",
    "y = train[\"toxic\"]  # 1 = tóxico, 0 = no tóxico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f265af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic text preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # pasar a minúsculas\n",
    "    text = text.lower()\n",
    "    # quitar URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # quitar caracteres especiales (excepto letras y números)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "X_clean = X.apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numbers (TF-IDF)\n",
    "# this is NLP, I found it online\n",
    "# i dont know how it works clearly, does anyone want to work on this together?\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X_vec = vectorizer.fit_transform(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate a simple model\n",
    "# these are different metrics used in machine learning\n",
    "# i saw them in a past course, if anyone wants me to help understand please let me know\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# modelo\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predicciones\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# métricas\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiar y vectorizar test\n",
    "X_test_clean = test[\"comment_text\"].apply(clean_text)\n",
    "X_test_vec = vectorizer.transform(X_test_clean)\n",
    "\n",
    "# predicciones en test\n",
    "test_pred = model.predict(X_test_vec)\n",
    "\n",
    "# crear DataFrame para inspección\n",
    "results_test = pd.DataFrame({\n",
    "    \"comment\": test[\"comment_text\"],\n",
    "    \"pred\": test_pred\n",
    "})\n",
    "\n",
    "# agregar columna 'pred_label' para mayor claridad\n",
    "results_test[\"pred_label\"] = results_test[\"pred\"].map({0: \"no_toxic\", 1: \"toxic\"})\n",
    "\n",
    "# mostrar primeras 10 filas con predicción\n",
    "print(results_test.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4522a7",
   "metadata": {},
   "source": [
    "# ANEXE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s']\", \"\", text)  # letters, spaces, '\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # adjust if it's French\n",
    "\n",
    "# Diccionario para unificar nombres de políticos\n",
    "# clave: posible forma en el texto, valor: nombre único\n",
    "politician_map = {\n",
    "    'marine le pen': 'marine_le_pen',\n",
    "    'le pen': 'marine_le_pen',\n",
    "    'pen': 'marine_le_pen',\n",
    "\n",
    "    'emmanuel macron': 'emmanuel_macron',\n",
    "    'macron': 'emmanuel_macron',\n",
    "\n",
    "    # agregá más políticos si hace falta\n",
    "}\n",
    "\n",
    "def replace_politicians(text):\n",
    "    for key, value in politician_map.items():\n",
    "        # reemplaza coincidencias exactas de la cadena\n",
    "        # agregamos espacios delante y detrás para no confundir palabras\n",
    "        pattern = r'\\b' + re.escape(key) + r'\\b'\n",
    "        text = re.sub(pattern, value, text)\n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = replace_politicians(text)  # primero normalizamos nombres\n",
    "    tokens = text.split()  # simple tokenize\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# aplicar al dataframe\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "df[[\"text\", \"clean_text\"]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
